---
title: "MLHW5_JF"
date: February 21, 2022
output: word_document
editor_options:
  chunk_output_type: console
---

```{r, include = FALSE}
library(tidyverse) 
library(caret)
library(glmnet)
library(klaR)
library(dplyr)
library(readxl)
library(knitr)
```

# QI: You should create and compare three different models. Remember to remove the ID variable as you do not want to include that in your analysis. 
```{r}
set.seed(100)

alc = read.csv("/Users/judyfordjuoh/Desktop/Machine Learning/ML_hw5/alcohol_use.csv")

#Strip off ID Variable
alc <- alc[,2:9]

#Make alcohol use into a alcohol consumption (0 for current use and 1 for current use) #From OH
alc$alc_consumption <- factor(alc$alc_consumption, levels = c("NotCurrentUse", "CurrentUse"))

#Check distributions, missing data etc.#From OH
summary(alc)

#Omit those with missing data #From OH
alc <- na.omit(alc)

#tidyverse way to create data partition (70/30)
#training.data<-chr$life_exp %>% createDataPartition(p=0.7, list=F)
train.indices <- createDataPartition(y = alc$alc_consumption,p = 0.7,list = FALSE)
train.data <- alc[train.indices, ]
test.data <- alc[-train.indices, ]

```

```{r} 
#REGULARIZED REGRESSION: ELASTIC NET
set.seed(123)

en.model <- train(
  alc_consumption ~., data = train.data, method = "glmnet",
  trControl = trainControl("cv", number = 10), preProc = c("center", "scale"), tuneLength = 10
  )
#Print the values of alpha and lambda that gave best prediction
en.model$bestTune %>% knitr::kable() # 0.6(alpha)| 0.2641193(lambda)| 0.8545881|(accuracy) 

#Print all of the options examined. Bc this is a logistic regression we are using the Accuracy. If it was linear regression it would be MSE/RMSE.
en.model$results %>% knitr::kable()

# Model coefficients
coef(en.model$finalModel, en.model$bestTune$lambda) 

#Confusion Matrix
confusionMatrix(en.model) 

```

```{r}
#LOGISTIC REGRESSION

logistic_control1 <- trainControl(method = "cv", number = 3, savePredictions = T)

set.seed(1000)
logistic <- train(alc_consumption ~ ., data = train.data, method = "glm", family = "binomial", trControl = logistic_control1)

summary(logistic)

confusionMatrix(logistic) 

confusionMatrix(table((logistic$pred)$pred,(logistic$pred)$obs))
```


```{r}
#LASSO

#NTS: first create a grid to search lambda
lambda <- 10^seq(-3,3, length = 100)

set.seed(100)

#NTS: replace tuneLength with tuneGrid and alpha is 1 because we are doing lasso. If we were doing rigid it would be 0. 
lasso_m <- train(
  alc_consumption ~., data = train.data, method = "glmnet", trControl = trainControl("cv", number = 10), preProc = c("center", "scale"), tuneGrid = expand.grid(alpha = 1, lambda = lambda)
)

#Print the values of alpha and lambda that gave best prediction
lasso_m$bestTune %>% knitr::kable() # 1(alpha)|0.23101(lambda)|0.8538 (Accuracy)

#Print all of the options examined
lasso_m$results %>% knitr::kable()

# Model coefficients
coef(lasso_m$finalModel, lasso_m$bestTune$lambda)

#Confusion Matrix
confusionMatrix(lasso_m) 

```


# Q2: Decide which model you would choose as your final model

```{r}
#Q2 Task: You should tune and compare the performance of all three models within the training set using cross-validation and then decide which model you would choose as your final model. Provide justification for your choice.

#Make a calibration plot to see which one is the best


#EN Calibration
fitted.results_modelen <- en.model %>% predict(train.data)

error_model_4 <- mean(fitted.results_modelen !=train.data$alc_consumption, na.rm = T)

print(paste('Accuracy [en.model]', 1-error_model_4))

testProb <- data.frame(obs = train.data$alc_consumption,
                        pred.logit = error_model_4)

calPlotData_model4<- calibration(obs~as.numeric(pred.logit), data = testProb)

xyplot(calPlotData_model4, auto.key = list(columns = 2))


#Logistic regression Calibration
fitted.results_model2 <- logistic %>% predict(test.data)

error_model2 <- mean(fitted.results_model2 !=test.data$alc_consumption, na.rm = T)

print(paste('Accuracy [logistic]', 1-error_model2))

testProb2 <- data.frame(obs = test.data$alc_consumption,
                        pred.logit2 = error_model2)

calPlotData_model2<- calibration(obs~as.numeric(pred.logit2), data = testProb2)

xyplot(calPlotData_model2, auto.key = list(columns = 2))


#LASSO Calibration
fitted.results_model5 <- lasso_m %>% predict(train.data)

error_model5 <- mean(fitted.results_model5 !=test.data$alc_consumption, na.rm = T)

print(paste('Accuracy [lasso_m]', 1-error_model5))

testProb5 <- data.frame(obs = test.data$alc_consumption,
                        pred.logit5 = error_model5)

calPlotData_model5<- calibration(obs~as.numeric(pred.logit5), data = testProb5)

xyplot(calPlotData_model5, auto.key = list(columns = 2))
```

In the elastic net model, the average accuracy was 0.8545. The intercept was 0.1365 and the remaining variables went to zero, except for Measure of Impulsivity (impulsiveness_score = 0.4253). The best predicting alpha and lambda was 0.6(alpha) and 0.2641(lambda), which resulted in an accuracy of 0.8545.

In the logistic regression model the average accuracy was 0.8152 and the sensitivity and specificity was 0.7942 and 0.8336, respectively.

In the LASSO model, the average accuracy was 0.8538. The intercept was 0.1336 and the remaining variables went to zero, except for Measure of Impulsivity (impulsiveness_score = 0.3038).The best predicting alpha and lambda was 1(alpha) and 0.2310(lambda), which resulted in an accuracy of 0.8545.

Both the elastic net model and the LASSO model had the same average accuracy, which was higher than the logistic regression, as well as the same calibration plot. Although it may seem like I can use either the elastic net or the lasso model for my final model, I am choosing the elastic net model because elastic Net combines characteristics of both lasso and ridge. Elastic Net reduces the impact of different features while not eliminating all of the features. The elastic net model also had a larger beta for impulsiveness_score compared to the LASSO model which can mean there is a relationship between this feature and the outcome which is worth further exploring. Also the coefficients in the elastic net model was slightly larger than the coefficients in the LASSO model.

# Q3: Apply your final model in the test set and report your final evaluation metrics
```{r}
#Using the test data to make predictions

en_pred2 <- en.model %>% predict(test.data)
confusionMatrix(en_pred2,test.data$alc_consumption, positive = "CurrentUse")

#Obtain predicted probabilities
test.outcome.probs<-predict(en.model, test.data, type="prob")

testProbs.rmodel <- data.frame(obs = test.data$alc_consumption,
                        pred.en=test.outcome.probs[,2])

#Create calibration plot
alc_PlotData.rmodel<-calibration(obs ~ pred.en, data = testProbs.rmodel, class="CurrentUse", cuts=5)

xyplot(alc_PlotData.rmodel, auto.key = list(columns = 2))

plot(test.outcome.probs[,2])

```
After applying my final model in the test set, the average accuracy was 0.8478 and the sensitivity and specificity was 1 and 0.6742, respectively. The positive predictive value and the negative predictive value were 0.778 and 1, respectively. Based on the calibration plot, we see that there are points where the two lines are very close, showing that the model has a good fit.

# Q5
```{r}
#Q5 Task: What research questions could this analysis either a) directly address or b) indirectly help to address by providing information that could be used in subsequent analyses? Limit this response to no more than 1 paragraph. Be sure to use complete sentences.
```
This research can be used for a plethora of research questions. This research can be used to directly address: does an individualâ€™s measure of impulsiveness affect their current use of alcohol? This research can be used to further indirectly help to address reaserch concerning the relationship between impulsiveness and sensation-seeking behaviors amongst NYC young adults (18-25 years old) and the rate of drunk driving deaths caused by NYC young adults.
