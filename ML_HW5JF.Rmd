---
title: "MLHW5_JF"
date: February 11, 2022
output: word_document
editor_options:
  chunk_output_type: console
---

```{r, include = FALSE}
library(tidyverse) 
library(caret)
library(glmnet)
library(klaR)
library(dplyr)
library(readxl)
library(knitr)
```

# QI: You should create and compare three different models. Remember to remove the ID variable as you do not want to include that in your analysis. 
```{r}
set.seed(100)

alc = read.csv("/Users/judyfordjuoh/Desktop/Machine Learning/ML_hw5/alcohol_use.csv")

#Strip off ID Variable
alc <- alc[,2:9]

#tidyverse way to create data partition (70/30)
#training.data<-chr$life_exp %>% createDataPartition(p=0.7, list=F)
train.indices <- createDataPartition(y = alc$alc_consumption,p = 0.7,list = FALSE)
train.data <- alc[train.indices, ]
test.data <- alc[-train.indices, ]

```

```{r} 
#REGULARIZED REGRESSION: ELASTIC NET
set.seed(123)

en.model <- train(
  alc_consumption ~., data = train.data, method = "glmnet",
  trControl = trainControl("cv", number = 10), preProc = c("center", "scale"), tuneLength = 10
  )
#Print the values of alpha and lambda that gave best prediction
en.model$bestTune %>% knitr::kable()

#Print all of the options examined. Bc this is a logistic regression we are using the Accuracy. If it was linear regression it would be MSE/RMSE.
en.model$results %>% knitr::kable()

# Model coefficients
coef(en.model$finalModel, en.model$bestTune$lambda) 

#Confusion Matrix
confusionMatrix(en.model) 

#This is the extra part idk if its needed of right
# Make predictions in test set
#en.pred = en.model %>% predict(test.data)

#NTS:Caret doesn't work if your binary response is of character/logical so you have to rename the levels as factors.
#test.data = test.data %>%
#mutate(alc_consumption = as.factor(alc_consumption))

# Model prediction performance

```

```{r}
#LOGISTIC REGRESSION

logistic_control1 <- trainControl(method = "cv", number = 3, savePredictions = T)

set.seed(1000)
logistic <- train(alc_consumption ~ ., data = alc, method = "glm", family = "binomial", trControl = logistic_control1)

summary(logistic)

confusionMatrix(logistic) 

confusionMatrix(table((logistic$pred)$pred,(logistic$pred)$obs))
```

```{r}
#LASSO

#NTS: first create a grid to search lambda
lambda <- 10^seq(-3,3, length = 100)

set.seed(100)

#NTS: replace tuneLength with tuneGrid and alpha is 1 because we are doing lasso. If we were doing rigid it would be 0. 
lasso_m <- train(
  alc_consumption ~., data = train.data, method = "glmnet", trControl = trainControl("cv", number = 10), preProc = c("center", "scale"), tuneGrid = expand.grid(alpha = 1, lambda = lambda)
)

#Print the values of alpha and lambda that gave best prediction
lasso_m$bestTune %>% knitr::kable()

#Print all of the options examined
lasso_m$results %>% knitr::kable()

# Model coefficients
coef(lasso_m$finalModel, lasso_m$bestTune$lambda)

#Confusion Matrix
confusionMatrix(lasso_m) 


# Make predictions in test set
#en.pred <- en.model %>% predict(test.data)

# Model prediction performance
#postResample(en.pred,test.data$life_exp) 

```


# Q2

```{r}
#Q2 Task: You should tune and compare the performance of all three models within the training set using cross-validation and then decide which model you would choose as your final model. Provide justification for your choice.

How do you know which one is the best? 
This can be done looking at the hyperparameter-specific results(i.e. finding the best tune values and then finding that in the results table (model$results)) seeing the performance metric for that value. Or you can use the confusion matric on the model object created by the train function. 

I would use the logistic regression because the false negatives would be lower? Nut the other two have a higher accuracy so I would go with those? I think I would use lasso bc of the feeautre selection and when you look at the accuracy of the best lambda, it is bigger for the lasso. The lasso also had only one vairable that did not go to zero and in the EN the other variable that did not go to zero had such a small change it could be neglibile.    .

```


# Q3
```{r}
#Q3 Task: Apply your final model in the test set and report your final evaluation metrics. 

#may have to apply/copy the # Make predictions in test set and the # Model prediction performance to here for the model I select
```


# Q4
```{r}
#Q4 Task: Produce a shareable report of your analysis and results using R Markdown.

```


# Q5
```{r}
#Q5 Task: What research questions could this analysis either a) directly address or b) indirectly help to address by providing information that could be used in subsequent analyses? Limit this response to no more than 1 paragraph. Be sure to use complete sentences.


```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
